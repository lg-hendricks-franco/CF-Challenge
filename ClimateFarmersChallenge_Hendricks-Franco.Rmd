---
title: "Lindsey's Climate Farmers Challenge"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

The work involves the use and analysis of gridded data (usually remote sensing or
model products), including land cover, climate and soil. You will need to download
and visualize relationships between these over a chosen region.

Load required packages
```{r}
# List of packages to load
packages_today <- c("tidyverse","ecmwfr","ggspatial","sf","rnaturalearth")

# Load the packages
lapply(packages_today, library, character.only = TRUE)

setwd("C:/Users/linds/Documents/Climate Farmers Challenge - Code/CF-Challenge")

```



_**Step 1: Region Selection**_

*Decide on a specific region to work with. This region should be large enough to
include various land use types but small enough to keep computation times
reasonable. A country or region within a country for example.
The region can be defined using simple longitude and latitude limits or using a
geospatial file (e.g. shape file or geojson). Use this region to filter downloaded data
in the next steps.*


First, I use the packages sf and rnaturalearth to access shape files of country and regional boundaries. I will work with the state of California, USA.
```{r}

# Get the natural earth data for U.S. states
california_sf <- ne_states(country = "united states of america", returnclass = "sf") %>% 
  filter(name == "California") %>% st_as_sf()
```


I confirm the correct geometry of the shape file and write to the computer, encoding with "UTF-8", as in the original file. 
```{r}
#Use base R to view state outline
plot(california_sf$geometry)

#Save shapefile
#st_write(california_sf, "california_sf.shp", driver = "ESRI Shapefile", encoding ="UTF-8")
```


I read back and plot the saved shape file to confirm correct writing to the drive.
```{r}
#Check file saved correctly by reading in shape file and plotting geometry
#test_shp<-st_read("california_sf.shp")
#plot(test_shp$geometry)
```


Finally, I plot on coordinates with ggplot2 to confirm the correct geographic position and begin building a graph for later visualisation.
```{r}
#Generate graph on coordinates to confirm position
ggplot(data = california_sf) +
  geom_sf() +
  ggtitle("California, USA") +
  theme(plot.title = element_text(hjust = 0.5)) +
  #xlab("") + ylab("") +
  geom_sf(color = "black", fill = "grey") 
```

_**Step 2: Data Acquisition**_

*Download data for your region for:
● Monthly evapotranspiration, temperature, and precipitation covering the
years 2000 to 2022 from (ERA5-Land):
https://cds.climate.copernicus.eu/#!/home
● Land cover classification maps for the year 2020 from Land Cover Classes
(Gridded Map):: https://cds.climate.copernicus.eu/#!/home
● Soil organic carbon (SOC) stock data: https://soilgrids.org/
If there are any questions about what exact variables should be used, use your
own judgment to make a choice.*

Comments on CDS functionality and data requests: https://cran.r-project.org/web/packages/ecmwfr/vignettes/cds_vignette.html


Ideally, I would have downloaded this data using the package 'ecmwfr'. First adding the keychain. Instead, for simplicity in completing the activity, I downloaded the data directly from the website using the built-in data navigator 
```{r}
lapply(c("ecmwfr","keyring"), library, character.only = TRUE)

ecmwfr::wf_set_key(user = "269199", key = "a107d4dc-ed19-46dc-ad85-390e61b50920", service = "cds")


```

Data Acquisition

● Monthly evapotranspiration, temperature, and precipitation covering the
years 2000 to 2022 from (ERA5-Land):
https://cds.climate.copernicus.eu/#!/home


Examine file with the raster package:

I chose to download total evaporation, soil temperature layer 1, and total precipitation at 12:00. Midday measurements will capture the transpiration component of evapotranspiration. During the default 0:00 measurements, the stomata of C3 plants are closed; thus no transpiration takes places. 

Comments on use of midday measurements:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8739081/

Shallow soils (layer 1) contain the organic horizon and are most dynamic in response to ambient temperature and precipitation.

I wrote a function to extract the data by climate variables, defining a brick for each one in the global environment. The new named brick files should be: stl1_brick, e_brick, tp_brick


```{r}
lapply(c("raster","ncdf4"), library, character.only = TRUE)

#read .nc file and convert to a brick - specify a variable
#varname: stl1, e, tp

nc_file_path <- "temp_evap_prec.nc"
variable_name <- "stl1"

read_var <- function(nc_file_path, variable_name) {

    var_name <- deparse(substitute(variable_name)) #unevaluated argument name, converted to character 
    
    clean_var_name <- gsub("\"", "", var_name) #remove double quotations
    
  # Read the NetCDF file for the specified variable
 #Use the cleaned variable name in the assign function
    assign(paste0(clean_var_name, "_brick"), brick(nc_file_path, varname = variable_name), envir = .GlobalEnv)

  #crop to CA and plot
  var_cropped <- crop(get(paste0(clean_var_name, "_brick")), extent(california_sf))
  var_masked <- mask(var_cropped, california_sf)
  plot(var_masked, main = paste0(clean_var_name, "_brick"))
}

read_var("temp_evap_prec.nc","stl1")
read_var("temp_evap_prec.nc","e")
read_var("temp_evap_prec.nc","tp")

```


*● Land cover classification maps for the year 2020 from Land Cover Classes
(Gridded Map):: https://cds.climate.copernicus.eu/#!/home*

Look at land cover classes from 2020

```{r}
landcover <- raster("landcover.nc")
#crop to CA and plot
var_cropped <- crop(landcover, extent(california_sf))
cal_cover <- mask(var_cropped, california_sf)
plot(cal_cover)

```
Interpret the land cover data. IPCC Classes on page 15:

https://datastore.copernicus-climate.eu/documents/satellite-land-cover/D5.3.1_PUGS_ICDR_LC_v2.1.x_PRODUCTS_v1.1.pdf

The most common classes are:

10 - Rainfed cropland
60 - Forest, Tree cover, broadleaved, deciduous, closed to open (> 15%)
110 - Grassland, Mosaic herbaceous cover (>50%) / tree and shrub (<50%)
120 - Shrubland

```{r}
library(sp)  #Load package for managing spatial data
unique(cal_cover$Land.cover.class.defined.in.LCCS)
hist(cal_cover$Land.cover.class.defined.in.LCCS)

```

For now, I am leaving out the soil carbon data due to time constraints. I may be able to return later to this task.


_**Step 3: Data Integration**_
*Reproject the different rasters to match the one with lowest resolution, thus allowing joint analysis.*

From this step forward, I will only work with the soil data brick 'stl1_brick' (shallow soil temperature). This allows me to show an example of the basic spatial data processing steps I could use here. Once this process is optimised, I would write a function to carry out the same steps on the other bricks. I could also combine the bricks into a stack and loop a function over the bricks in the stack. 

I used the raster package to check the resolution of the land cover raster and the soil temperature brick.

```{r}
#Confirm resolution of each data set

res(cal_cover)
proj4string(cal_cover)

res(stl1_brick)
proj4string(stl1_brick)


```



Now I will reproject the land cover data to the lower resolution of the ERA5 climate data. The 'resample' function from the 'raster' package estimates a new value for the lower resolution data, based on a defined method. I chose 'nearest neighbor', which is suitable for preserving the categorical data of land cover classes.

```{r}
# Resample the higher resolution raster to the resolution of the lower resolution raster
cal_cover_res <- resample(cal_cover, stl1_brick, method = "ngb")
res(cal_cover_res)
plot(cal_cover_res)
unique(cal_cover_res)

```


Graph the high- and low-resolution histograms of land cover classes to check distribution in both projections. Note that a few rare land cover classes were lost in resampling, but the main categories are retained:

10 - Rainfed cropland
60 - Forest, Tree cover, broadleaved, deciduous, closed to open (> 15%)
110 - Grassland, Mosaic herbaceous cover (>50%) / tree and shrub (<50%)
120 - Shrubland

Unique values of land cover class are:
 [1]  10  11  30  40  70  80  90 110 120 130 160 180 190 200 210

```{r}

par(mfrow=c(1,2))
hist(cal_cover$Land.cover.class.defined.in.LCCS)
hist(cal_cover_res$Land.cover.class.defined.in.LCCS)
par(mfrow=c(1,1))
```


_**Step 4: Analysis and Visualization**_
*Visualize the time series of climate and soil variables in the format of your choice,
aggregated separately for different land cover classes.*

Due to time constraints, I am only providing a basic time series visualisation of the soil temperature data, aggregated by land cover class. As mentioned above, this could be further written as a function to efficiently analyses other bricks in the same fasion or to loop across the bricks in a stack. 

```{r}

#head(cal_cover_res)
#head(stl1_brick)


# Convert the brick to a data frame

stl1_df <- rasterToPoints(stl1_brick) %>% as.data.frame()
cover_df <- rasterToPoints(cal_cover_res) %>% as.data.frame()

#Join the data frames bases on x,y coordinates (lat,lon) so that the land cover and soil temperature data coorespond spatially.

full_df <- full_join(cover_df, stl1_df, by = c("x","y")) %>% na.omit() %>% 
  rename(cover_class = "Land.cover.class.defined.in.LCCS") %>%
  mutate(cover_class = as.factor(cover_class))

stl1_summ <- full_df[3:39.] %>%
  gather(2:37,key="date",value="value") %>%
  mutate(cover_class = recode(cover_class,
                              "10" = "10 - Rainfed Cropland",
                              "11" = "10 - Rainfed Cropland",
                              "30" = "30 - Mosaic Cropland",
                              "40" = "40 - Mosaic Natural Vegetation",
                              "70" = "70 - Tree cover, needleleaved, evergreen",
                              "80" = "80 - Tree cover, needleleaved, deciduous",
                              "90" = "90 - Tree cover, mixed leaf type",
                              "110" = "110 - Mosaic herbaceous cover",
                              "120" = "120 - Shrubland", "130" = "130 - Grassland",
                              "160" = "160 - Tree cover, flooded",
                              "180" = "180 - Shrub/herbaceous cover, flooded",
                              "190" = "190 - Urban Areas", "200" = "200 - Bare Areas", "210" = "210 - Water Bodies")) %>%
  mutate(date = substr(as.character(date), start = 2, stop = nchar(as.character(date)) - 12)) %>%
  mutate(date = as.factor(date)) %>%
  group_by(cover_class, date) %>% 
  summarise(mn.val = mean(value), se = sd(value)/sqrt(length(value))) %>% 
  as.data.frame()

head(stl1_summ)


# Plot the time series using ggplot2
stl1_plot <- ggplot(data=stl1_summ, aes(x=date, y=mn.val, group = cover_class)) + 
  geom_line(aes(color=cover_class), linewidth=.3, position = position_dodge(width=0)) +
  #geom_errorbar(aes(ymin=mn.val-se,ymax=mn.val+se, color = cover_class), size=.3, width=.3, position = position_dodge(width=0)) +
  ylab(expression("Soil Temperature (K)")) +
  xlab("Date") + theme_bw() +
  ggtitle("California Shallow Soil Temperature by Land Cover Class") +
  theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust = 1, size = 7))

stl1_plot
```


_**Step 5: Sampling Design**_
*Imagine you wanted to detect changes in SOC in the region in response to farm
management practices or some other factor. so you need a sampling plan. Based
on the SOC data you obtained, determine the number of samples that would
need to be taken in the region in order to have a 95% confidence interval equal or
less than 10% of the mean value. Assume a Gaussian distribution.*

Due to time constraints, I did not calculate this minimum required sample size from the SOC data. This would be possible to calculate from the formula for sample size, assuming a Gaussian (normal) distribution. According the the central limit theorem, the mean distribution of a sample will tend toward normality, as long as the sample size is high enough; therefore, the assumption of a Gaussian distribution is appropriate for large spatial data sets.

sample size = (Z^2 x sd^2)/(E^2)

Z = The Z-score for the 95% confidence interval
sd = Estimated standard deviation, derived from the sample
E = The desired margin of error


Alternatively, I will calculate the required samples size required for the same error parameters, but for data from soil temperature (stl1_brick) at a given time (May 2022).

```{r}
# Given error parameters
conf_int <- 0.95  # 95% confidence interval
Z <- qnorm((1 + conf_int) / 2)  # Z-score for the given confidence level
margin_of_error <- 0.10  # Desired margin of error - 10% of the mean

# Estimate standard deviation
est_sd <- sd(as.vector(stl1_brick$X2022.05.01.12.00.00), na.rm = TRUE)

# Calculate required sample size
sample_size <- (Z^2 * est_sd^2) / (margin_of_error^2)

# Round up to the nearest integer
sample_size <- ceiling(sample_size)

# Print the result
print(sample_size)

```


